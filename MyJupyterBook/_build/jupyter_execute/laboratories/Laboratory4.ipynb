{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f90b0ca1",
   "metadata": {},
   "source": [
    "# Laboratory Task 4\n",
    "\n",
    "Genheylou Felisilda - DS4A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17058a18",
   "metadata": {},
   "source": [
    "Instruction: Train a linear regression model in PyTorch using a regression dataset. Use the following parameters. <br>\n",
    "\n",
    "- Criterion: MSE Loss\n",
    "- Fully Connected Layers x 2\n",
    "- Batch Size: 8\n",
    "- Optimizer: SGD\n",
    "- Epoch: 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42962973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import load_diabetes\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c41b01d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the diabetes dataset\n",
    "X, y = load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea11165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to PyTorch tensors\n",
    "\n",
    "X_tensor = torch.FloatTensor(X)\n",
    "y_tensor = torch.FloatTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9403f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader with batch size of 8\n",
    "\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a8d0bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model with 2 fully connected layers\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)  # First FC layer\n",
    "        self.fc2 = nn.Linear(64, 1)           # Second FC layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = LinearRegressionModel(X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7df9de77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define MSE loss and SGD optimizer\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad9803de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 6911.6172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/1000], Loss: 12506.8369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/1000], Loss: 702.2333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [400/1000], Loss: 7069.4341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [500/1000], Loss: 1008.1329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [600/1000], Loss: 10351.6445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [700/1000], Loss: 366.3314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [800/1000], Loss: 3868.9624\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39mTraceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m num_epochs = \u001b[32m1000\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Zero gradients\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Forward pass\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:494\u001b[39m, in \u001b[36mDataLoader.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    492\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:424\u001b[39m, in \u001b[36mDataLoader._get_iterator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_iterator\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> _BaseDataLoaderIter:\n\u001b[32m    423\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_workers == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_SingleProcessDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    425\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    426\u001b[39m         \u001b[38;5;28mself\u001b[39m.check_worker_number_rationality()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:768\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter.__init__\u001b[39m\u001b[34m(self, loader)\u001b[39m\n\u001b[32m    767\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, loader):\n\u001b[32m--> \u001b[39m\u001b[32m768\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    769\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._timeout == \u001b[32m0\u001b[39m\n\u001b[32m    770\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_workers == \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__init__\u001b[39m\u001b[34m(self, loader)\u001b[39m\n\u001b[32m    697\u001b[39m \u001b[38;5;28mself\u001b[39m._collate_fn = loader.collate_fn\n\u001b[32m    698\u001b[39m \u001b[38;5;28mself\u001b[39m._sampler_iter = \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m._index_sampler)\n\u001b[32m    699\u001b[39m \u001b[38;5;28mself\u001b[39m._base_seed = (\n\u001b[32m    700\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m     .item()\n\u001b[32m    703\u001b[39m )\n\u001b[32m    704\u001b[39m \u001b[38;5;28mself\u001b[39m._persistent_workers = loader.persistent_workers\n\u001b[32m    705\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded = \u001b[32m0\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# train for 1000 epochs\n",
    "\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_X, batch_y in dataloader:\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs.squeeze(), batch_y)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Print progress every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3675ba",
   "metadata": {},
   "source": [
    "Loss is fluctuating significantly\n",
    " The loss jumps up and down across epochs rather than steadily decreasing. This indicates that the network is not converging properly and the learning process is unstable.\n",
    "\n",
    "Potential causes of instability:\n",
    "- Randomness of data processing\n",
    "- Learning rate might be too high, causing the weights to overshoot the minimum.\n",
    "- The network architecture or initialization may not be suitable for the data.\n",
    "- The dataset might be small or noisy, making it hard for the network to find a consistent pattern.\n",
    "- Training is not effective: After 1000 epochs, the loss is still very high, which implies the network has not learned to approximate the target outputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}