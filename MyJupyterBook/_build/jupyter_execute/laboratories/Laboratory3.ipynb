{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7835ce6-404c-4770-a811-19826c1c565f",
   "metadata": {},
   "source": [
    "# Laboratory Task 3\n",
    "\n",
    "Genheylou Felisilda - DS4A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4751c312",
   "metadata": {},
   "source": [
    "Instruction: Perform a forward and backward propagation in python using the inputs from Laboratory Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eef96f9-231d-4b99-9cb1-4d729bc6faef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9574abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs and target\n",
    "x = np.array([1, 0, 1])\n",
    "y = np.array([1])\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c9d0f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights and biases (from Task 2)\n",
    "W_hidden = np.array([[0.2, 0.4, -0.5],   # weights for hidden neuron 1\n",
    "                     [-0.3, 0.1, 0.2]])  # weights for hidden neuron 2\n",
    "b_hidden = np.array([-0.4, 0.2])         # biases for hidden layer\n",
    "\n",
    "W_output = np.array([-0.3, -0.2])        # weights for output neuron\n",
    "b_output = 0.1                            # bias for output neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6f16c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation function\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "470bfc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivative of ReLU\n",
    "def relu_derivative(z):\n",
    "    return np.where(z > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ece0d19",
   "metadata": {},
   "source": [
    "### Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a4a9ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden layer\n",
    "z_hidden = np.dot(W_hidden, x) + b_hidden\n",
    "a_hidden = relu(z_hidden)\n",
    "\n",
    "# Output layer\n",
    "z_output = np.dot(W_output, a_hidden) + b_output\n",
    "a_output = relu(z_output)\n",
    "\n",
    "# Compute error\n",
    "error = 0.5 * (y - a_output)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4938a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Propagation Results:\n",
      "Hidden layer pre-activation: [-0.7  0.1]\n",
      "Hidden layer activation: [0.  0.1]\n",
      "Output pre-activation: 0.08\n",
      "Predicted output: 0.08\n",
      "Error: [0.4232]\n"
     ]
    }
   ],
   "source": [
    "print(\"Forward Propagation Results:\")\n",
    "print(\"Hidden layer pre-activation:\", z_hidden)\n",
    "print(\"Hidden layer activation:\", a_hidden)\n",
    "print(\"Output pre-activation:\", z_output)\n",
    "print(\"Predicted output:\", a_output)\n",
    "print(\"Error:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76203dc2",
   "metadata": {},
   "source": [
    "### Backward Propagation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08b670aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output layer delta\n",
    "delta_output = (a_output - y) * relu_derivative(z_output)\n",
    "\n",
    "# Hidden layer delta\n",
    "delta_hidden = delta_output * W_output * relu_derivative(z_hidden)\n",
    "\n",
    "# Update output weights and bias\n",
    "W_output -= lr * delta_output * a_hidden\n",
    "b_output -= lr * delta_output\n",
    "\n",
    "# Update hidden weights and biases\n",
    "W_hidden -= lr * np.outer(delta_hidden, x)\n",
    "b_hidden -= lr * delta_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e2af57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Backward Propagation Updates:\n",
      "Updated hidden weights:\n",
      " [[ 0.2       0.4      -0.5     ]\n",
      " [-0.300184  0.1       0.199816]]\n",
      "Updated hidden biases: [-0.4       0.199816]\n",
      "Updated output weights: [-0.3      -0.199908]\n",
      "Updated output bias: [0.10092]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBackward Propagation Updates:\")\n",
    "print(\"Updated hidden weights:\\n\", W_hidden)\n",
    "print(\"Updated hidden biases:\", b_hidden)\n",
    "print(\"Updated output weights:\", W_output)\n",
    "print(\"Updated output bias:\", b_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4a5ec2",
   "metadata": {},
   "source": [
    "The network adjusts its weights and biases gradually to minimize error, illustrating that learning is iterative. The ReLU activation guides which signals influence updates, emphasizing how activation functions affect learning. This demonstrates that even simple networks rely on repeated forward and backward passes to approximate target outputs effectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}